# âœ… ì‹¤í–‰ ì‹œ ê³ ì •ëŒ“ê¸€ â†’ ë”ë³´ê¸°ë€ê¹Œì§€ë§Œ í™•ì¸í•˜ê³  ìŠ¤í¬ë¦½íŠ¸ëŠ” ìƒëµí•œ ë²„ì „

import json
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
import requests
import logging
import sys
import os
from dotenv import load_dotenv
import re
from datetime import datetime
from langdetect import detect

log_date = datetime.now().strftime("%Y-%m-%d")
log_path = f"logs/menu_extraction_{log_date}.log"
logging.basicConfig(
    filename=log_path,
    level=logging.INFO,
    format="%(asctime)s - %(message)s",
)

class DualLogger:
    def __init__(self):
        self.terminal = sys.__stdout__
    def write(self, message):
        try:
            self.terminal.write(message)
        except:
            self.terminal.write(message.encode("utf-8", "ignore").decode("utf-8"))
        logging.info(message.strip())
    def flush(self):
        self.terminal.flush()

sys.stdout = DualLogger()

def sanitize(text):
    if not isinstance(text, str):
        return str(text)
    return ''.join(c for c in text if not (0xD800 <= ord(c) <= 0xDFFF))

def safe_print(msg):
    print(sanitize(msg))

load_dotenv()
API_KEY = os.getenv("YOUTUBE_API_KEY")
SONAR_API_KEY = os.getenv("SONAR_API_KEY")
CHANNEL_ID = "UC0N7H8ALIQSnktDH6wy7iSw"

def extract_json_block(text):
    try:
        match = re.search(r'\{[\s\S]*?\}', text)
        if match:
            parsed = json.loads(match.group())
            if "menu" in parsed and "ingredients" in parsed:
                return parsed
    except Exception as e:
        # safe_print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        safe_print(f"âŒ Failed to parse JSON: {e}")

    return None

def get_existing_urls(file_path="src/menuTest.js"):
    if not os.path.exists(file_path):
        return set()
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
        urls = re.findall(r'"url":\s*"([^"]+)",?', content)
        return set(urls)

def initialize_js_file_if_needed(file_path="src/menuTest.js"):
    if not os.path.exists(file_path):
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("const menuTest = [\n];\n\nexport default menuTest;\n")

def append_to_js(parsed_data, video_url, uploader_name, upload_date, file_path="src/menuTest.js"):
    try:
        entry = {
            "name": parsed_data["menu"],
            "url": video_url,
            "uploader": uploader_name,
            "upload_date": upload_date,
            "ingredients": parsed_data["ingredients"],
            "source": parsed_data.get("source", "unknown")
        }
        if not os.path.exists(file_path):
            initialize_js_file_if_needed(file_path)

        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            existing_items = re.findall(r'\{[\s\S]*?\}', content)
            for item in existing_items:
                try:
                    data = json.loads(item)
                    if data.get("url") == entry["url"]:
                        # safe_print("âš ï¸ ì´ë¯¸ ì €ì¥ëœ URL â†’ ì¶”ê°€ ìƒëµ")
                        safe_print("âš ï¸ URL already exists â†’ Skipping insertion")

                        return
                except:
                    continue
            lines = content.splitlines()
            close_idx = next((i for i, line in reversed(list(enumerate(lines))) if line.strip().startswith("]")), -1)
            export_idx = next((i for i, line in reversed(list(enumerate(lines))) if "export default" in line), -1)
            if close_idx == -1 or export_idx == -1:
                # safe_print("âŒ JS í˜•ì‹ ì´ìƒ")
                safe_print("âŒ Invalid JS file format")

                return
            insert_idx = 1
            new_line = json.dumps(entry, ensure_ascii=False, indent=2) + ","
            lines.insert(insert_idx, new_line)
            with open(file_path, "w", encoding="utf-8") as f:
                f.writelines(line + "\n" for line in lines)
        #safe_print(f"âœ… ë°ì´í„° ì¶”ê°€ ì™„ë£Œ (ì¶œì²˜: {entry['source']})")
        safe_print(f"âœ… Data added successfully (Source: {entry['source']})")

        safe_print(json.dumps(entry, ensure_ascii=False, indent=2))
    except Exception as e:
        # safe_print(f"âŒ JS ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        safe_print(f"âŒ Error while saving JS file: {e}")


def finalize_js_file(file_path="src/menuTest.js"):
    try:
        with open(file_path, "r+", encoding="utf-8") as f:
            content = f.read().rstrip(",\n")
            f.seek(0)
            f.write(content)
            f.truncate()
        # safe_print("ğŸ“ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
        safe_print("ğŸ“ Finalizing data write completed")

    except Exception as e:
        # safe_print(f"âŒ ë°ì´í„° ì‹¤íŒ¨: {e}")
        safe_print(f"âŒ Error while finalizing file: {e}")


def get_video_ids_and_channel(api_key, channel_id, max_results=0):
    youtube = build("youtube", "v3", developerKey=api_key)
    videos = []
    search_response = youtube.search().list(
        channelId=channel_id,
        part="id",
        order="date",
        maxResults=max_results,
        type="video"
    ).execute()
    video_ids = [item["id"]["videoId"] for item in search_response["items"]]
    video_response = youtube.videos().list(
        part="snippet,contentDetails",
        id=",".join(video_ids)
    ).execute()
    for item in video_response["items"]:
        if item["snippet"]["liveBroadcastContent"] != "none":
            continue
        duration = item["contentDetails"]["duration"]
        match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration)
        hours = int(match.group(1) or 0)
        minutes = int(match.group(2) or 0)
        seconds = int(match.group(3) or 0)
        total_seconds = hours * 3600 + minutes * 60 + seconds
        if total_seconds >= 30 * 60:
            # safe_print(f"â© {item['id']} â†’ ì˜ìƒ ê¸¸ì´ {total_seconds//60}ë¶„ â†’ ê±´ë„ˆëœ€")
            safe_print(f"â© {item['id']} â†’ Video too long ({total_seconds//60} min) â†’ Skipping")

            continue
        videos.append((item["id"], item["snippet"]["channelId"]))
    return videos

def get_first_comment_and_author(api_key, video_id):
    youtube = build("youtube", "v3", developerKey=api_key)
    response = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=1,
        textFormat="plainText"
    ).execute()
    if response.get("items"):
        comment_snippet = response["items"][0]["snippet"]["topLevelComment"]["snippet"]
        return comment_snippet["textDisplay"], comment_snippet["authorChannelId"]["value"]
    return None, None

def get_description(youtube, video_id):
    try:
        response = youtube.videos().list(part="snippet", id=video_id).execute()
        return response["items"][0]["snippet"]["description"]
    except Exception as e:
        # safe_print(f"âŒ ë”ë³´ê¸°ë€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        safe_print(f"âŒ Failed to fetch description: {e}")

        return None
        



def ask_sonar_from_comment(comment_text, source_name=""):
    headers = {
        "Authorization": f"Bearer {SONAR_API_KEY}",
        "Content-Type": "application/json"
    }
    prompt = f"""ë‚´ìš©ì—ì„œ ìš”ë¦¬ ë©”ë‰´ ì´ë¦„ê³¼ ì¬ë£Œë“¤ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- ë‹¤ì§„/ê¹/ì‚¶ì€ ë“±ì˜ ìˆ˜ì‹ì–´ëŠ” ì œê±°í•˜ê³  ì¬ë£Œ ì´ë¦„ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”. ì˜ˆ) ê¹ë§ˆëŠ˜ â†’ ë§ˆëŠ˜, ë‹¤ì§„ ìª½íŒŒ â†’ ìª½íŒŒ
- ë©”ë‰´ë‚˜ ì¬ë£Œê°€ ì—†ê³  ì œí’ˆ ì„¤ëª…ì´ë‚˜ í™ë³´ë§Œ ìˆë‹¤ë©´ \"Only ì œí’ˆ ì„¤ëª… OR í™ë³´\"ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”.
- ì„œë¸Œ ë©”ë‰´ê°€ ìˆê±°ë‚˜ ì—¬ëŸ¬ ë©”ë‰´ê°€ ìˆì–´ë„ ë©”ë‰´ëŠ” ë©”ì¸ ë©”ë‰´ëŠ” í•˜ë‚˜ì´ë©°, ë‘˜ë‹¤ ë©”ì¸ ê°™ìœ¼ë©´ ë©”ì¸ íƒ€ì´í‹€ ê°™ì€ê±¸ ì“°ê±°ë‚˜ ì´ë¦„ì„ ì ë‹¹íˆ í•©ì³ì¤˜. ê·¸ë¦¬ê³  ëª¨ë“  ì¬ë£ŒëŠ” ì¤‘ë³µ ì—†ì´ \"ì¬ë£Œ\"ì— í†µí•©í•´ì£¼ì„¸ìš”.
- ì¬ë£Œ ì´ë¦„ê³¼ ë„ì–´ì“°ê¸°ë„ ì˜¬ë°”ë¥´ê²Œ í•´ì¤˜
- ì¬ë£Œ ëŒ€ì²´: ìƒìˆ˜ëŠ” ë¬¼ë¡œ ëŒ€ì²´í•´. ì—‘ìŠ¤íŠ¸ë¼ ë²„ì§„ ì˜¬ë¦¬ë¸Œì˜¤ì¼ì€ ê·¸ëƒ¥ ì˜¬ë¦¬ë¸Œì˜¤ì¼ë¡œ ëŒ€ì²´. íŒŒìŠ¤íƒ€ë©´ ì¢…ë¥˜ëŠ” ê·¸ëƒ¥ íŒŒìŠ¤íƒ€ë¼ê³  ëŒ€ì²´í•´ì¤˜. ì¦‰ì„ë°¥, í–‡ë°˜, ë°±ë¯¸ ê°™ì€ê±°ëŠ” ê·¸ëƒ¥ ë°¥ìœ¼ë¡œ ëŒ€ì²´. ì½”ì¸ìœ¡ìˆ˜ëŠ” ìˆëŠ” ê·¸ëŒ€ë¡œ í•´ì¤˜. ex) ê½ƒê²Œì½”ì¸ìœ¡ìˆ˜ -> ê½ƒê²Œì½”ì¸ìœ¡ìˆ˜.

Please extract the cooking menu name and ingredients in JSON format from the content.

- Remove adjectives like "chopped", "peeled", or "boiled" and keep only the ingredient names.  
  e.g., "peeled garlic" â†’ "garlic", "chopped scallions" â†’ "scallions"
- If there is no menu or ingredient and the content is only product promotion or explanation, return: "Only ì œí’ˆ ì„¤ëª… OR í™ë³´"
- If there are sub-menus or multiple main dishes, assume there is only one main menu.  
  If both seem like the main, choose a main title or combine the names appropriately.
- Combine all ingredients into one unified "ingredients" array with no duplicates.
- Use proper spacing and accurate ingredient naming (e.g., "soy sauce", not "soysauce").
- Ingredient normalization:  
  "mineral water" â†’ "water"  
  "extra virgin olive oil" â†’ "olive oil"  
  Any pasta variety â†’ "pasta"  
  "instant rice", "microwave rice", "white rice" â†’ "rice"  
  Leave coin broth names as-is (e.g., "crab coin broth" stays the same).


ë‚´ìš©:
{sanitize(comment_text)}

í˜•ì‹:
{{
  \"menu\": \"menu name\",
  \"ingredients\": [\"ingredient1\", \"ingredient2\", ...]
}}"""
    payload = {
        "model": "sonar-reasoning-pro",
        "messages": [
            {"role": "system", "content": "ë„Œ ìš”ë¦¬ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì•¼."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "search": False
    }
    try:
        response = requests.post("https://api.perplexity.ai/chat/completions", headers=headers, json=payload)
        if response.status_code == 200:
            return sanitize(response.json()["choices"][0]["message"]["content"])
        else:
            # safe_print(f"âŒ Sonar ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
            safe_print(f"âŒ Sonar API request failed with status: {response.status_code}")

    except Exception as e:
        # safe_print(f"âŒ Sonar ìš”ì²­ ì˜¤ë¥˜: {e}")
        safe_print(f"âŒ Sonar request error: {e}")

    return None

def run():
    videos_all = get_video_ids_and_channel(API_KEY, CHANNEL_ID, max_results=50)
    videos = videos_all[:5]
    youtube = build("youtube", "v3", developerKey=API_KEY)
    file_path = "src/menuTest.js"
    existing_urls = get_existing_urls(file_path)
    initialize_js_file_if_needed(file_path)

    for idx, (video_id, uploader_id) in enumerate(videos, start=1):
        video_url = f"https://youtu.be/{video_id}"
        if video_url in existing_urls:
            # safe_print(f"âš ï¸ ì´ë¯¸ ì €ì¥ëœ URL â†’ {video_url} â†’ ê±´ë„ˆëœ€")
            safe_print(f"âš ï¸ URL already exists â†’ Skipping: {video_url}")
            continue
        video_response = youtube.videos().list(part="snippet", id=video_id).execute()
        snippet = video_response["items"][0]["snippet"]
        uploader_name = snippet["channelTitle"]
        upload_date = snippet["publishedAt"][:10]
        comment, author_id = get_first_comment_and_author(API_KEY, video_id)

        sources = [
            ("Pinned Comment", comment if author_id == uploader_id else None),
            ("Description Box", get_description(youtube, video_id))
        ]
        for source_name, text in sources:
            if not text:
                continue
            result = ask_sonar_from_comment(text, source_name)
            parsed = extract_json_block(result)
            if parsed:
                parsed["source"] = source_name
                append_to_js(parsed, video_url, uploader_name, upload_date, file_path)
                break

    finalize_js_file(file_path)

run()
