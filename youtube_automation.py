# âœ… ì‹¤í–‰ ì‹œ ê³ ì •ëŒ“ê¸€ â†’ ë”ë³´ê¸°ë€ê¹Œì§€ë§Œ í™•ì¸í•˜ê³  ìŠ¤í¬ë¦½íŠ¸ëŠ” ìƒëµí•œ ë²„ì „
# í˜¸ìœ¤ì†Šì…ë‹ˆë‹¤ ì œë°œã…‡

import json  
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
import requests
import logging
import sys
import os
from dotenv import load_dotenv
import re
from datetime import datetime
import socket

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.auth.transport.requests
import google.auth

import socket
import httplib2

socket.setdefaulttimeout(10)


log_date = datetime.now().strftime("%Y-%m-%d")
log_path = f"logs/menu_extraction_{log_date}.log"

logging.basicConfig(
    filename=log_path,
    level=logging.INFO,
    format="%(asctime)s - %(message)s",
)

class DualLogger:
    def __init__(self):
        self.terminal = sys.__stdout__
    def write(self, message):
        try:
            self.terminal.write(message)
        except:
            self.terminal.write(message.encode("utf-8", "ignore").decode("utf-8"))
        logging.info(message.strip())
    def flush(self):
        self.terminal.flush()

sys.stdout = DualLogger()

# âœ… ìœ ë‹ˆì½”ë“œ ì—ëŸ¬ ë°©ì§€
def sanitize(text):
    if not isinstance(text, str):
        return str(text)
    return ''.join(c for c in text if not (0xD800 <= ord(c) <= 0xDFFF))

def safe_print(msg):
    print(sanitize(msg))

load_dotenv()
API_KEY = os.getenv("YOUTUBE_API_KEY")
SONAR_API_KEY = os.getenv("SONAR_API_KEY")
CHANNEL_ID = "UC0x63Jy1Sy63grrf_Pq0WEg"

def extract_json_block(text):
    try:
        match = re.search(r'\{[\s\S]*?\}', text)
        if match:
            parsed = json.loads(match.group())
            if "ë©”ë‰´" in parsed and "ì¬ë£Œ" in parsed:
                return parsed
    except Exception as e:
        safe_print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    return None

def get_existing_urls(file_path="src/menuData_kr.js"):
    if not os.path.exists(file_path):
        return set()
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
        urls = re.findall(r'"url":\s*"([^"]+)"', content)
        return set(urls)

def append_to_js(parsed_data, video_url, uploader_name, upload_date, file_path="src/menuData_kr.js"):
    try:
        entry = {
            "name": parsed_data["ë©”ë‰´"],
            "url": video_url,
            "uploader": uploader_name,
            "upload_date": upload_date,
            "ingredients": parsed_data["ì¬ë£Œ"],
            "source": parsed_data.get("ì¶œì²˜", "unknown")
        }

        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            existing_items = re.findall(r"\{[\s\S]*?\}", content)
            for item in existing_items:
                try:
                    data = json.loads(item)
                    if data.get("url") == entry["url"]:
                        safe_print("âš ï¸ ì´ë¯¸ ì €ì¥ëœ URL â†’ ì¶”ê°€ ìƒëµ")
                        return
                except:
                    continue
            lines = content.splitlines()

                # ğŸ”§ ìˆ˜ì • ì „:
        close_idx = next((i for i, line in reversed(list(enumerate(lines))) if line.strip() == "]"), -1)

        # âœ… ìˆ˜ì • í›„:
        close_idx = next((i for i, line in reversed(list(enumerate(lines))) if line.strip().startswith("]")), -1)

        export_idx = next((i for i, line in reversed(list(enumerate(lines))) if "export default" in line), -1)
        if close_idx == -1 or export_idx == -1:
            safe_print("âŒ JS í˜•ì‹ ì´ìƒ")
            return

        insert_idx = 1
        lines.insert(insert_idx, json.dumps(entry, ensure_ascii=False, indent=2) + ",\n")
        with open(file_path, "w", encoding="utf-8") as f:
            f.writelines(line + "\n" for line in lines)

        safe_print(f"âœ… ë°ì´í„° ì¶”ê°€ ì™„ë£Œ (ì¶œì²˜: {entry['source']})")
    except Exception as e:
        safe_print(f"âŒ JS ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

def initialize_js_file_if_needed(file_path="src/menuData_kr.js"):
    if not os.path.exists(file_path):
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("const menuData_kr = [\n];\n\nexport default menuData_kr;\n")

def finalize_js_file(file_path="src/menuData_kr.js"):
    try:
        with open(file_path, "r+", encoding="utf-8") as f:
            content = f.read().rstrip(",\n")
            f.seek(0)
            f.write(content)
            f.truncate()
        safe_print("ğŸ“ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
    except Exception as e:
        safe_print(f"âŒ ë°ì´í„° ì‹¤íŒ¨: {e}")



def get_video_ids_and_channel(api_key, channel_id, max_results=200):
    youtube = build("youtube", "v3", developerKey=api_key)
    videos = []
    next_page_token = None

    while len(videos) < max_results:
        search_response = youtube.search().list(
            channelId=channel_id,
            part="id",
            order="date",
            maxResults=50,
            type="video",
            pageToken=next_page_token
        ).execute()

        video_ids = [item["id"]["videoId"] for item in search_response["items"]]
        video_response = youtube.videos().list(
            part="snippet,contentDetails",
            id=",".join(video_ids)
        ).execute()

        for item in video_response["items"]:
            snippet = item.get("snippet", {})
            live_status = snippet.get("liveBroadcastContent", "none")
            duration = item["contentDetails"]["duration"]

            if live_status != "none":
                continue

            match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration)
            hours = int(match.group(1) or 0)
            minutes = int(match.group(2) or 0)
            seconds = int(match.group(3) or 0)
            total_seconds = hours * 3600 + minutes * 60 + seconds

            if total_seconds >= 1800:
                video_url = f"https://youtu.be/{item['id']}"
                if video_url in get_existing_urls():
                    safe_print(f"âš ï¸ ì´ë¯¸ ì €ì¥ëœ URL â†’ {video_url} â†’ ê±´ë„ˆëœ€ (ê¸¸ì´ ê¹€)")
                else:
                    safe_print(f"â© {item['id']} â†’ ì˜ìƒ ê¸¸ì´ {total_seconds//60}ë¶„ â†’ ì €ì¥ (ê¸¸ì´ ê¹€)")
                    append_to_js(
                        {
                            "ë©”ë‰´": "ê±´ë„ˆëœ€ - ì˜ìƒ ë„ˆë¬´ ê¹€",
                            "ì¬ë£Œ": [],
                            "ì¶œì²˜": "ìë™ íŒë³„"
                        },
                        video_url,
                        item["snippet"]["channelTitle"],
                        item["snippet"]["publishedAt"][:10]
                    )
                continue


            videos.append((item["id"], item["snippet"]["channelId"]))

        next_page_token = search_response.get("nextPageToken")
        if not next_page_token:
            break

    return videos[:max_results]


def get_first_comment_and_author(api_key, video_id):
    youtube = build("youtube", "v3", developerKey=api_key)
    response = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=1,
        textFormat="plainText"
    ).execute()
    if response.get("items"):
        comment_snippet = response["items"][0]["snippet"]["topLevelComment"]["snippet"]
        return comment_snippet["textDisplay"], comment_snippet["authorChannelId"]["value"]
    return None, None

def get_description(youtube, video_id):
    try:
        response = youtube.videos().list(part="snippet", id=video_id).execute()
        return response["items"][0]["snippet"]["description"]
    except Exception as e:
        safe_print(f"âŒ ë”ë³´ê¸°ë€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

def ask_sonar_from_comment(comment_text, source_name=""):
    headers = {
        "Authorization": f"Bearer {SONAR_API_KEY}",
        "Content-Type": "application/json"
    }

    prompt_prefix = {
        "ê³ ì •ëŒ“ê¸€": "ì´ ëŒ“ê¸€ì€ ìœ íŠœë¸Œ ìš”ë¦¬ ì˜ìƒì˜ ê³ ì • ëŒ“ê¸€ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.",
        "ë”ë³´ê¸°ë€": "ì´ í…ìŠ¤íŠ¸ëŠ” ìœ íŠœë¸Œ ì˜ìƒì˜ ë”ë³´ê¸°ë€ì…ë‹ˆë‹¤. ë©”ë‰´/ì¬ë£Œì™€ ë¬´ê´€í•˜ê±°ë‚˜ ê´‘ê³ , ì œí’ˆ í™ë³´, ë§í¬ ì•ˆë‚´ê°€ ì£¼ëœ ê²½ìš° ë¶„ì„í•˜ì§€ ë§ê³  'ë¶„ì„ ë¶ˆê°€'ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”."
    }

    prompt = f"""{prompt_prefix.get(source_name, '')}

ë‚´ìš©ì—ì„œ ìš”ë¦¬ ë©”ë‰´ ì´ë¦„ê³¼ ì¬ë£Œë“¤ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- ë‹¤ì§„/ê¹/ì‚¶ì€ ë“±ì˜ ìˆ˜ì‹ì–´ëŠ” ì œê±°í•˜ê³  ì¬ë£Œ ì´ë¦„ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”. ì˜ˆ) ê¹ë§ˆëŠ˜ â†’ ë§ˆëŠ˜, ë‹¤ì§„ ìª½íŒŒ â†’ ìª½íŒŒ
- ë©”ë‰´ë‚˜ ì¬ë£Œê°€ ì—†ê³  ì œí’ˆ ì„¤ëª…ì´ë‚˜ í™ë³´ë§Œ ìˆë‹¤ë©´ \"Only ì œí’ˆ ì„¤ëª… OR í™ë³´\"ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”.
- ì„œë¸Œ ë©”ë‰´ê°€ ìˆê±°ë‚˜ ì—¬ëŸ¬ ë©”ë‰´ê°€ ìˆì–´ë„ ë©”ë‰´ëŠ” ë©”ì¸ ë©”ë‰´ëŠ” í•˜ë‚˜ì´ë©°, ë‘˜ë‹¤ ë©”ì¸ ê°™ìœ¼ë©´ ë©”ì¸ íƒ€ì´í‹€ ê°™ì€ê±¸ ì“°ê±°ë‚˜ ì´ë¦„ì„ ì ë‹¹íˆ í•©ì³ì¤˜. ê·¸ë¦¬ê³  ëª¨ë“  ì¬ë£ŒëŠ” ì¤‘ë³µ ì—†ì´ \"ì¬ë£Œ\"ì— í†µí•©í•´ì£¼ì„¸ìš”.
- ì¬ë£Œ ì´ë¦„ê³¼ ë„ì–´ì“°ê¸°ë„ ì˜¬ë°”ë¥´ê²Œ í•´ì¤˜
- ì¬ë£Œ ëŒ€ì²´: ìƒìˆ˜ëŠ” ë¬¼ë¡œ ëŒ€ì²´í•´. ì—‘ìŠ¤íŠ¸ë¼ ë²„ì§„ ì˜¬ë¦¬ë¸Œì˜¤ì¼ì€ ê·¸ëƒ¥ ì˜¬ë¦¬ë¸Œì˜¤ì¼ë¡œ ëŒ€ì²´. íŒŒìŠ¤íƒ€ë©´ ì¢…ë¥˜ëŠ” ê·¸ëƒ¥ íŒŒìŠ¤íƒ€ë¼ê³  ëŒ€ì²´í•´ì¤˜. ì¦‰ì„ë°¥, í–‡ë°˜, ë°±ë¯¸ ê°™ì€ê±°ëŠ” ê·¸ëƒ¥ ë°¥ìœ¼ë¡œ ëŒ€ì²´. ì½”ì¸ìœ¡ìˆ˜ëŠ” ìˆëŠ” ê·¸ëŒ€ë¡œ í•´ì¤˜. ex) ê½ƒê²Œì½”ì¸ìœ¡ìˆ˜ -> ê½ƒê²Œì½”ì¸ìœ¡ìˆ˜.
- ì˜ìƒ í•˜ë‚˜ì— ì—¬ëŸ¬ê°€ì§€ì˜ ë‹¤ë¥¸ ìš”ë¦¬ë¥¼ í•˜ëŠ”ê±° ê°™ìœ¼ë©´ ê° ë©”ë‰´ì™€ ì¬ë£Œë¥¼ ë”°ë¡œ ì¶”ê°€í•´ì¤˜.  
ë‚´ìš©:
{sanitize(comment_text)}

í˜•ì‹:
{{
  \"ë©”ë‰´\": \"ë©”ë‰´ ì´ë¦„\",
  \"ì¬ë£Œ\": [\"ì¬ë£Œ1\", \"ì¬ë£Œ2\", ...]
}}"""

    payload = {
        "model": "sonar-reasoning-pro",
        "messages": [
            {"role": "system", "content": "ë„Œ ìš”ë¦¬ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì•¼."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "search": False
    }

    response = requests.post("https://api.perplexity.ai/chat/completions", headers=headers, json=payload)
    if response.status_code == 200 and response.content.strip():
        try:
            return sanitize(response.json()["choices"][0]["message"]["content"])
        except Exception as e:
            safe_print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    else:
        safe_print(f"âŒ Sonar ì‘ë‹µ ì—†ìŒ ë˜ëŠ” ì‹¤íŒ¨: {response.status_code}")
        return None

# âœ… ì‹¤í–‰ ë¶€ë¶„
videos_all = get_video_ids_and_channel(API_KEY, CHANNEL_ID, max_results=200)
videos = videos_all[:50]
existing_urls = get_existing_urls()
youtube = build("youtube", "v3", developerKey=API_KEY)
initialize_js_file_if_needed()

for idx, (video_id, uploader_id) in enumerate(videos, start=1):
    video_url = f"https://youtu.be/{video_id}"
    if video_url in existing_urls:
        safe_print(f"âš ï¸ ì´ë¯¸ ì €ì¥ëœ URL â†’ {video_url} â†’ ê±´ë„ˆëœ€")
        continue

    safe_print(f"\nğŸ“Œ ì˜ìƒ {idx}ë²ˆ: {video_url}")
    video_response = youtube.videos().list(part="snippet", id=video_id).execute()
    snippet = video_response["items"][0]["snippet"]
    uploader_name = snippet["channelTitle"]
    upload_date = snippet["publishedAt"][:10]
    comment, author_id = get_first_comment_and_author(API_KEY, video_id)

    sources = [
        ("ê³ ì •ëŒ“ê¸€", comment if author_id == uploader_id else None),
        ("ë”ë³´ê¸°ë€", get_description(youtube, video_id))
    ]

    for source_name, text in sources:
        safe_print(f"â­ï¸ í˜„ì¬ ë‹¨ê³„: {source_name} í™•ì¸ ì¤‘...")
        if not text:
            safe_print(f"ğŸš« {source_name} ì—†ìŒ ë˜ëŠ” í™•ì¸ ë¶ˆê°€ â†’ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™")
            continue
        safe_print(f"ğŸ“„ {source_name} ë¶„ì„ ì‹œë„")
        result = ask_sonar_from_comment(text, source_name)
        safe_print(f"ğŸ§  Sonar ì‘ë‹µ ({source_name}):\n{result}")

        parsed = extract_json_block(result)
        if parsed:
            parsed["ì¶œì²˜"] = source_name
            append_to_js(parsed, video_url, uploader_name, upload_date)
            break

        else:
            safe_print(f"âš ï¸ {source_name} ë¶„ì„ ì‹¤íŒ¨ â†’ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™")
            failed_entry = {
                "ë©”ë‰´": "ë¶„ì„ ë¶ˆê°€",
                "ì¬ë£Œ": [],
                "ì¶œì²˜": source_name
            }
            append_to_js(failed_entry, video_url, uploader_name, upload_date)
            break

    safe_print("-" * 60)

finalize_js_file()