import json  
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
import requests
import logging
import sys
import os
from dotenv import load_dotenv
import re

# ë¡œê·¸ + í„°ë¯¸ë„ ë™ì‹œ ì¶œë ¥
logging.basicConfig(
    filename="logs/menu_extraction.log",
    level=logging.INFO,
    format="%(asctime)s - %(message)s",
)
class DualLogger:
    def __init__(self):
        self.terminal = sys.__stdout__
    def write(self, message):
        self.terminal.write(message)
        logging.info(message.strip())
    def flush(self):
        self.terminal.flush()
sys.stdout = DualLogger()

# í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
API_KEY = os.getenv("YOUTUBE_API_KEY")
SONAR_API_KEY = os.getenv("SONAR_API_KEY")
CHANNEL_ID = "UC2IIBYSTMSvJaK2UJzCC06g"

def extract_json_block(text):
    try:
        match = re.search(r'\{[\s\S]*?\}', text)
        if match:
            parsed = json.loads(match.group())
            if "ë©”ë‰´" in parsed and "ì¬ë£Œ" in parsed:
                return parsed
    except Exception as e:
        print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨:", e)
    return None
    
def append_to_js(parsed_data, video_url, uploader_name, upload_date, file_path="src/menuData_kr.js"):
    try:
        entry = {
            "name": parsed_data["ë©”ë‰´"],
            "url": video_url,
            "uploader": uploader_name,
            "upload_date": upload_date,
            "ingredients": parsed_data["ì¬ë£Œ"]
        }

        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        # ë°°ì—´ ë‹«ëŠ” ìœ„ì¹˜ ì°¾ê¸° (ë§¨ ë§ˆì§€ë§‰ export ì œì™¸ ì „ì˜ ]; ìœ„ì¹˜)
        close_idx = next((i for i, line in reversed(list(enumerate(lines))) if line.strip() == "];"), -1)
        export_idx = next((i for i, line in reversed(list(enumerate(lines))) if "export default" in line), -1)

        if close_idx == -1 or export_idx == -1:
            print("âŒ JS í˜•ì‹ ì´ìƒ: ë‹«ëŠ” ê´„í˜¸ë‚˜ export ì¤„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return

        # ìƒˆ ë°ì´í„° ì‚½ì… ìœ„ì¹˜ëŠ” ë°°ì—´ ì‹œì‘ ë°”ë¡œ ë’¤ (const ë‹¤ìŒ ì¤„)
        insert_idx = 1  # const menuData_kr = [ ë‹¤ìŒ ì¤„
        json_str = json.dumps(entry, ensure_ascii=False, indent=2)
        lines.insert(insert_idx, json_str + ",\n")

        # ë®ì–´ì“°ê¸°
        with open(file_path, "w", encoding="utf-8") as f:
            f.writelines(lines)

        print("âœ… ë°ì´í„° ì¶”ê°€ ì™„ë£Œ (ë‹«ê¸°/export ì¤„ì€ ìˆ˜ì • ì•ˆ í•¨)")
    except Exception as e:
        print("âŒ JS ì €ì¥ ì¤‘ ì˜¤ë¥˜:", e)


def initialize_js_file_if_needed(file_path="src/menuData_kr.js"):
    if not os.path.exists(file_path):
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("const menuData_kr = [\n];\n\nexport default menuData_kr;\n")
    else:
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        # ì¤‘ë³µëœ export ì œê±°
        new_lines = []
        seen_export = False
        for line in lines:
            if "export default" in line:
                if not seen_export:
                    seen_export = True
                    new_lines.append(line)
            else:
                new_lines.append(line)

        with open(file_path, "w", encoding="utf-8") as f:
            f.writelines(new_lines)


def finalize_js_file(file_path="src/menuData_kr.js"):
    try:
        with open(file_path, "r+", encoding="utf-8") as f:
            content = f.read().rstrip(",\n")  # ë§ˆì§€ë§‰ ì½¤ë§ˆ ì œê±°
            f.seek(0)
            f.write(content)
            f.truncate()
        print("ğŸ“ JS íŒŒì¼ ì¢…ë£Œ êµ¬ë¬¸ ì¶”ê°€ ì™„ë£Œ")
    except Exception as e:
        print("âŒ ì¢…ë£Œ êµ¬ë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨:", e)

def get_video_ids_and_channel(api_key, channel_id, max_results=0):
    youtube = build("youtube", "v3", developerKey=api_key)
    videos = []
    search_response = youtube.search().list(
        channelId=channel_id,
        part="id",
        order="date",
        maxResults=max_results,
        type="video"
    ).execute()
    video_ids = [item["id"]["videoId"] for item in search_response["items"]]
    video_response = youtube.videos().list(
        part="snippet",
        id=",".join(video_ids)
    ).execute()
    for item in video_response["items"]:
        if item["snippet"]["liveBroadcastContent"] == "none":
            videos.append((item["id"], item["snippet"]["channelId"]))
    return videos

def get_first_comment_and_author(api_key, video_id):
    youtube = build("youtube", "v3", developerKey=api_key)
    response = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=1,
        textFormat="plainText"
    ).execute()
    if response.get("items"):
        comment_snippet = response["items"][0]["snippet"]["topLevelComment"]["snippet"]
        text = comment_snippet["textDisplay"]
        author_id = comment_snippet["authorChannelId"]["value"]
        return text, author_id
    return None, None

def ask_sonar_from_comment(comment_text):
    headers = {
        "Authorization": f"Bearer {SONAR_API_KEY}",
        "Content-Type": "application/json"
    }
    prompt = f"""ì´ ëŒ“ê¸€ì€ ìœ íŠœë¸Œ ìš”ë¦¬ ì˜ìƒì˜ ê³ ì • ëŒ“ê¸€ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ëŒ“ê¸€ì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ë¦¬ ë©”ë‰´ ì´ë¦„ê³¼ ì¬ë£Œë“¤ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. 
    ë§Œì•½ ëŒ“ê¸€ì´ ë©”ë‰´ë‚˜ ì¬ë£Œì™€ ë¬´ê´€í•˜ê±°ë‚˜, ì œí’ˆ í™ë³´ë‚˜ ì•ˆë‚´ì¼ ê²½ìš° ë¶„ì„í•˜ì§€ ë§ê³  "ë¶„ì„ ë¶ˆê°€"ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”. 
    ì¬ë£Œì˜ ì–‘ì€ í•„ìš”ì—†ìœ¼ë©°, ìƒìˆ˜ëŠ” ë¬¼ë¡œ ëŒ€ì²´. ì–‘ë…ì¥/ë“œë ˆì‹±ì´ ì—¬ëŸ¬ ì¬ë£Œë¡œ êµ¬ì„±ë˜ë©´ êµ¬ì„± ì„±ë¶„ë„ í¬í•¨í•´ì£¼ì„¸ìš”.
    ë‹¨, ë‹¤ì§„/ê¹/ì‚¶ì€ ë“±ì˜ ìˆ˜ì‹ì–´ëŠ” ì œê±°í•˜ê³  ì¬ë£Œ ì´ë¦„ë§Œ í¬í•¨í•´ì£¼ì„¸ìš”. Ex) ê¹ë§ˆëŠ˜ â†’ ë§ˆëŠ˜, ë‹¤ì§„ ìª½íŒŒ â†’ ìª½íŒŒ

ëŒ“ê¸€:
{comment_text}

í˜•ì‹:
{{
  "ë©”ë‰´": "ë©”ë‰´ ì´ë¦„",
  "ì¬ë£Œ": ["ì¬ë£Œ1", "ì¬ë£Œ2", ...]
}}"""
    payload = {
        "model": "sonar-reasoning",
        "messages": [
            {"role": "system", "content": "ë„Œ ìš”ë¦¬ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì•¼."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "search": False
    }
    response = requests.post("https://api.perplexity.ai/chat/completions", headers=headers, json=payload)

    if response.status_code == 200 and response.content.strip():
        try:
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            print("âŒ Sonar ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜:", e)
            return None
    else:
        print("âŒ Sonar ì‘ë‹µ ì—†ìŒ ë˜ëŠ” ì‹¤íŒ¨:", response.status_code)
        return None

# ì‹¤í–‰
videos_all = get_video_ids_and_channel(API_KEY, CHANNEL_ID, max_results=50)
videos = videos_all[30:40]

youtube = build("youtube", "v3", developerKey=API_KEY)
initialize_js_file_if_needed()

for idx, (video_id, uploader_id) in enumerate(videos, start=1):
    print(f"\nğŸ“Œ ì˜ìƒ {idx}ë²ˆ: https://youtu.be/{video_id}")
    comment, author_id = get_first_comment_and_author(API_KEY, video_id)

    if comment and author_id == uploader_id:
        print("âœ… ê³ ì • ëŒ“ê¸€ í™•ì¸ë¨ â†’ Sonar ë¶„ì„ ì‹œì‘")
        result = ask_sonar_from_comment(comment)
        print("ğŸ§  Sonar ì‘ë‹µ:\n", result)

        parsed = extract_json_block(result)

        if parsed:
            video_response = youtube.videos().list(part="snippet", id=video_id).execute()
            snippet = video_response["items"][0]["snippet"]
            uploader_name = snippet["channelTitle"]
            upload_date = snippet["publishedAt"][:10]
            video_url = f"https://youtu.be/{video_id}"

            append_to_js(parsed, video_url, uploader_name, upload_date)
        else:
            print("âš ï¸ Sonar ë¶„ì„ ì‹¤íŒ¨ ë˜ëŠ” ë¶„ì„ ëŒ€ìƒ ì•„ë‹˜ â†’ ìƒëµ")
    else:
        print("âŒ ê³ ì • ëŒ“ê¸€ ì—†ìŒ â†’ ë¶„ì„ ìƒëµ")

    print("-" * 60)

finalize_js_file()
